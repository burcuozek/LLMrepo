{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb56b95a",
   "metadata": {},
   "source": [
    "# Document Based - Question Answering using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c7322-be84-42a8-938f-72631fb3bd46",
   "metadata": {},
   "source": [
    "RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.\n",
    "\n",
    "RAG is useful for question-answering tasks where the model needs to provide informative and contextually relevant responses. By combining retrieval and generation techniques, RAG aims to achieve a balance between factual accuracy and creative language generation.\n",
    "\n",
    "This notebook presents a question-answering system utilizing Large Language Models (LLM). The Langchain framework and Hugging Face were employed. [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was used as an embedding method, which is a sentence-transformers model and facilitates the mapping of sentences and paragraphs into a 384-dimensional dense vector space. Additionally, for the generator model, the Flan [T5](https://huggingface.co/docs/transformers/model_doc/t5) model was utilized, which has an encoder-decoder transformer architecture that uses a text-to-text approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bef9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related libraries and packages\n",
    "\n",
    "# !pip install unstructured_inference\n",
    "# !pip install pikepdf\n",
    "# !pip install pypdf\n",
    "\n",
    "import torch\n",
    "from langchain.document_loaders import UnstructuredPDFLoader # pip install unstructured\n",
    "from transformers import pipeline as hfpipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline as hfpipeline\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc36795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "                separator = \"\\n\\n\",\n",
    "                chunk_size = 1000,\n",
    "                chunk_overlap  = 200,\n",
    "                length_function = len,\n",
    "        )\n",
    "\n",
    "# Choose the model to create embeddings\n",
    "retriever = SentenceTransformer('all-MiniLM-L6-v2') # text - vector\n",
    "\n",
    "# Read the pdf file\n",
    "# Paper: Ozek et all - Analysis of pain research literature through keyword Co-occurrence networks\n",
    "loader = UnstructuredPDFLoader(\"KCN_PLOSDigitalHealth.pdf\") # \n",
    "data = loader.load()\n",
    "\n",
    "# Preprocess text\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "replace_newline = lambda text: text.replace('\\n', ' ')\n",
    "replace_dash_space = lambda text: text.replace('- ', '')\n",
    "\n",
    "modified_texts_temp = [ replace_dash_space(replace_newline(doc.page_content)) for doc in docs]\n",
    "\n",
    "# Convert documents to sentences\n",
    "modified_texts = []\n",
    "for paragraph in modified_texts_temp:\n",
    "    modified_texts.extend(nltk.sent_tokenize(paragraph))\n",
    "\n",
    "# Create embeddings\n",
    "corpus_embeddings = retriever.encode(modified_texts, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7404ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specifically, machine learning and biomarkers have become one of the leading research topics. (Score: 0.5406)\n",
      "\n",
      "It shows an  increasing trend with time. (Score: 0.5147)\n",
      "\n",
      "On the other hand, there is no such upward trend in the time windows 2002–2006 and 2007–2011; the trend is flat, but the average weighted nearest neighbor’s degree fluctuates with the degree. (Score: 0.4684)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input query\n",
    "query = \"Is Machine learning trending?\"\n",
    "\n",
    "# Encode query into vector space \n",
    "query_embedding = retriever.encode(query, convert_to_tensor=True)\n",
    "# Use cosine similarity\n",
    "cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0] #\n",
    "\n",
    "# Find relevant top three passages (vectors)\n",
    "top_results = torch.topk(cos_scores, k=3)\n",
    "\n",
    "corpus_results = []\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    print(modified_texts[idx], \"(Score: {:.4f})\\n\".format(score))\n",
    "    corpus_results.append(modified_texts[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "763a24d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d723b134df004b2fa9ce0c4a86b82a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47277cd440e14a059fa528dc6e9363f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953e0d93f3ac44caaa89a5e58f6d33e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dfe267b0f24bfba93977c6fdd0cfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11852dc29622493d91589a0e4969eea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334019582d0f4bb5950ea758fc2e3591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617cc66c935142d6ae90ac82383ee528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose generator \n",
    "# T5 encoder-decoder transformer as the LLM will answer the query based on context\n",
    "sentence_generator_model = 'google/flan-t5-base'\n",
    "task = 'text2text-generation'\n",
    "\n",
    "generator = hfpipeline(task, model=sentence_generator_model) # model default t5-base - we use google/flan-t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36643ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted query: \n",
      " Answer the following question based on the context. Question: Is Machine learning trending? context: Specifically, machine learning and biomarkers have become one of the leading research topics.  It shows an  increasing trend with time.  On the other hand, there is no such upward trend in the time windows 2002–2006 and 2007–2011; the trend is flat, but the average weighted nearest neighbor’s degree fluctuates with the degree. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burcuozek/opt/anaconda3/envs/LLM/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: [{'generated_text': 'an increasing trend'}]\n"
     ]
    }
   ],
   "source": [
    "# Create the format of the prompt\n",
    "context = [f\"{m} \" for m in corpus_results]\n",
    "context = \" \".join(context)\n",
    "query_formatted = f\"Answer the following question based on the context. Question: {query} context: {context}\"\n",
    "\n",
    "print(\"Formatted query: \\n\" , query_formatted)\n",
    "\n",
    "answer = generator(query_formatted)\n",
    "print(\"\\n\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012ff17-7fdb-421d-a7cb-2d8775249b50",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://research.ibm.com/blog/retrieval-augmented-generation-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb70e4-150f-43f7-b4f9-77dc2f293078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
